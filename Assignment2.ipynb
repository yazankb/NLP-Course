{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIgM6C9HYUhm"
   },
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
    "\n",
    "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
    "\n",
    "Useful links:\n",
    "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
    "- [Norvig's dataset](https://norvig.com/big.txt)\n",
    "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
    "\n",
    "Grading:\n",
    "- 60 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-vb8yFOGRDF"
   },
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "You may also want to implement:\n",
    "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
    "- some recent (or not very recent) paper on this topic,\n",
    "- solution which takes into account keyboard layout and associated misspellings,\n",
    "- efficiency improvement to make the solution faster,\n",
    "- any other idea of yours to improve the Norvig’s solution.\n",
    "\n",
    "IMPORTANT:  \n",
    "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
    "- Your implementation\n",
    "- Analysis of why the implemented approach is suggested\n",
    "- Improvements of the original approach that you have chosen to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1044268it [01:30, 11497.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm.models import KneserNeyInterpolated\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# prepare data\n",
    "df = pd.read_csv(\"fivegrams.txt\", sep=\"\\t\", header=None, names=['frequency', 'word1', 'word2', 'word3', 'word4', 'word5'])\n",
    "sequences = []\n",
    "frequencies = []\n",
    "for _, row in tqdm(df.iterrows()):\n",
    "    frequency = row['frequency']\n",
    "    ngram = [row['word1'], row['word2'], row['word3'], row['word4'], row['word5']]\n",
    "    sequences.extend([ngram] * 1)\n",
    "    frequencies.extend([frequency] * 1)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MoQeEsZvHvvi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of 'indigestion' given context '('a', 'baby', 'in', 'the')': 1.0418361103558039e-10\n"
     ]
    }
   ],
   "source": [
    "order = 5\n",
    "# Manually add padding tokens at the beginning of each sequence\n",
    "sequences_padded = [['<s>'] * (order - 1 ) + seq for seq in sequences[:100000]]\n",
    "train_data_padded, vocab = padded_everygram_pipeline(order, sequences_padded)\n",
    "\n",
    "# Initialize and train the Kneser-Ney interpolated model\n",
    "kn_model = KneserNeyInterpolated(order)\n",
    "kn_model.fit(train_data_padded, vocab)\n",
    "\n",
    "# Example usage: scoring a word given a context\n",
    "context = ('a', 'baby', 'in','the')\n",
    "word = 'indigestion'\n",
    "score = kn_model.score(word, context)\n",
    "print(f\"Score of '{word}' given context '{context}': {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of 'baby' given context '('a', 'baby', 'in', 'new')': 1.0507223787618395e-05\n"
     ]
    }
   ],
   "source": [
    "context = ('a', 'kid', 'in','new')\n",
    "word = 'baby'\n",
    "score = kn_model.score(word, context)\n",
    "print(f\"Score of '{word}' given context '{context}': {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of 'york' given context '('a', 'baby', 'in', 'new')': 0.7077801584613578\n"
     ]
    }
   ],
   "source": [
    "context = ('a', 'baby', 'in','new')\n",
    "word = 'york'\n",
    "score = kn_model.score(word, context)\n",
    "print(f\"Score of '{word}' given context '{context}': {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return list(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    edits1_result = edits1(word)\n",
    "    edits2_result = (edits1(e1) for e1 in edits1_result)\n",
    "    \n",
    "    edits2_in_vocab = (edit for edit in edits2_result if edit in vocab)\n",
    "    \n",
    "    return list(edits2_in_vocab)\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return edits2(word) + edits1(word) + [word]\n",
    "\n",
    "\n",
    "\n",
    "def P(word, context=('<s>', '<s>', '<s>', '<s>')):  \n",
    "    adjusted_context = context[-(order-1):] if len(context) >= order-1 else ('<s>',)*(order-1-len(context)) + tuple(context)\n",
    "    # Calculate and return the score of the word given its context\n",
    "    return kn_model.score(word, adjusted_context)\n",
    "\n",
    "def correction(word, context=('<s>', '<s>', '<s>', '<s>')): \n",
    "    \n",
    "    # Generate possible spelling corrections for the word\n",
    "    return max(candidates(word), key=lambda w: P(w, context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'house'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction(\"horse\", context=('a', 'baby', 'in','his'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction(\"tima\", context=('student', 'does', \"n't\",'have'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oML-5sJwGRLE"
   },
   "source": [
    "## Justify your decisions\n",
    "\n",
    "### Kneser-Ney Interpolated Model\n",
    "\n",
    "Effectively addresses the challenge of sparse data, which is common in language modeling, by distributing probability mass to unseen n-grams in a nuanced manner.\n",
    "\n",
    "Context Sensitivity: Leverages the context provided by preceding words to predict the likelihood of the next word, crucial for tasks requiring understanding of word usage in context, such as spell correction.\n",
    "\n",
    "Robustness and Generalization: Offers better generalization to new texts by dealing effectively with zero-frequency problems, making the model robust across diverse inputs.\n",
    "\n",
    "Interpolation for Enhanced Estimates: Uses interpolation to combine probabilities from different n-gram levels, utilizing both specific and broader context for more accurate probability estimates.\n",
    "\n",
    "Empirical Success: Has been empirically shown to perform well across various datasets and tasks, establishing it as a preferred choice for language modeling.\n",
    "\n",
    "### Choice of 5-gram (Quint-grams)\n",
    "Increased Context Sensitivity: Provides a richer contextual basis for predictions by considering the four preceding words, improving the model’s ability to capture linguistic nuances.\n",
    "\n",
    "Balance Between Specificity and Generalization: Offers a middle ground that captures sufficient context for usefulness while avoiding overfitting, thus maintaining the model’s ability to generalize from training data.\n",
    "\n",
    "Computational Feasibility: Represents a practical upper limit for n-gram size, balancing the benefits of additional context against the computational and memory requirements.\n",
    " \n",
    " Diminishing Returns Beyond 5-grams: Acknowledges that while higher n-grams could provide more context, the performance improvement may not justify the increased computational complexity.\n",
    "\n",
    "\n",
    "These points illustrate the thoughtful considerations behind selecting the Kneser-Ney Interpolated model and a 5-gram approach, aiming to optimize for context sensitivity, computational efficiency, and model effectiveness in handling the nuances of natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46rk65S4GRSe"
   },
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OwZWaX9VVs7B"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "def generate_misspelled_word(word):\n",
    "    possible_edits = list(edits1(word)) + list(edits2(word))\n",
    "    if not possible_edits:\n",
    "        return word\n",
    "    return random.choice(possible_edits)\n",
    "def report_accuracy(df):\n",
    "    total_words = 0\n",
    "    successful_corrections = 0\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Split sentence into words\n",
    "        words = row['Sentence'].lower().split()\n",
    "        \n",
    "        # Iterate over each word in the sentence\n",
    "        for word in words:\n",
    "            total_words += 1\n",
    "            context = [\"<s>\"]\n",
    "            # Generate misspelled word\n",
    "            misspelled_word = generate_misspelled_word(word)\n",
    "            \n",
    "            # Correct misspelled word\n",
    "            corrected_word = correction(misspelled_word, context)\n",
    "            context.append(corrected_word)\n",
    "            # Check if corrected word matches original word\n",
    "            if corrected_word == word:\n",
    "                successful_corrections += 1\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = successful_corrections / total_words if total_words > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "df = pd.read_csv(\"custom_data.csv\", encoding = \"latin-1\")\n",
    "report_accuracy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
